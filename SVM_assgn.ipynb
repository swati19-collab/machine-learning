{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "SVM & Naive Bayes | Assignment"
      ],
      "metadata": {
        "id": "4ubUlgT45c5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "Ans: A Support Vector Machine (SVM) is a powerful and versatile supervised machine learning algorithm used for both classification and regression tasks. Its primary objective is to find the optimal decision boundary, known as a hyperplane, that separates data points of different classes.\n",
        "\n",
        "The key idea behind SVM is to not just separate the classes, but to do so with the largest possible margin. The margin is the distance between the hyperplane and the closest data points from each class. These closest points are called support vectors, and they play a crucial role in defining the hyperplane.\n",
        "\n",
        "\n",
        "\n",
        "How SVM Works\n",
        "Finding the Optimal Hyperplane: SVM's goal is to find a hyperplane that maximizes the margin between the two classes. A larger margin generally leads to better generalization on new, unseen data, as it provides a wider \"buffer zone\" between the classes.\n",
        "\n",
        "\n",
        "Support Vectors: The data points that are closest to the hyperplane and essentially \"support\" or define the margin are the support vectors. If you were to remove any of these support vectors, the position and orientation of the hyperplane would change. All other data points have no influence on the final boundary.\n",
        "\n",
        "\n",
        "\n",
        "Handling Non-Linear Data with the Kernel Trick: Many real-world datasets aren't linearly separable, meaning you can't draw a straight line (or flat hyperplane) to divide the classes. For these cases, SVM uses a technique called the kernel trick. The kernel trick implicitly maps the data into a higher-dimensional space where it can be linearly separated.  By applying a kernel function (like the Radial Basis Function or RBF), SVM can find a non-linear decision boundary in the original space, effectively handling complex relationships between data points without the computational cost of explicitly transforming the data.\n",
        "\n",
        "Soft vs. Hard Margin:\n",
        "\n",
        "Hard Margin SVM: This approach assumes the data is perfectly separable and aims for a perfect separation with no misclassifications. It works well when the data is clean and there are no outliers.\n",
        "\n",
        "\n",
        "Soft Margin SVM: This is a more flexible approach that allows for some misclassifications or points to be on the wrong side of the margin. It introduces a regularization parameter (C) that balances the trade-off between maximizing the margin and minimizing the number of classification errors. This makes it more robust to noisy data and outliers."
      ],
      "metadata": {
        "id": "w5SjrJEQ5hFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X8wFbGG_57DL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM\n",
        "Ans:The primary difference between Hard Margin and Soft Margin SVM lies in how they handle data that isn't perfectly separable.\n",
        "\n",
        "Hard Margin SVM\n",
        "A Hard Margin SVM seeks to find a hyperplane that perfectly separates the data points of different classes. This means there can be no misclassifications, and all data points must be on the correct side of the margin.\n",
        "\n",
        "\n",
        "When to use: This approach is only feasible when the data is linearly separable—meaning a single straight line or flat hyperplane can perfectly divide the classes—and there are no outliers or noise.\n",
        "\n",
        "Key characteristic: It's very sensitive to outliers. A single misplaced data point can make it impossible to find a separating hyperplane, causing the algorithm to fail.\n",
        "\n",
        "\n",
        "Analogy: Imagine trying to separate two different colors of marbles on a table with a ruler. A Hard Margin SVM is like insisting on placing the ruler so that none of the marbles are on the wrong side or even touching the ruler.\n",
        "\n",
        "Soft Margin SVM\n",
        "A Soft Margin SVM is a more flexible and practical approach that allows for some misclassifications or data points to be within the margin. This is achieved by introducing a penalty term for these violations.\n",
        "\n",
        "\n",
        "When to use: This is the more common and robust approach, used for real-world data that often contains noise, outliers, or overlapping classes.\n",
        "\n",
        "Key characteristic: It introduces a regularization parameter (C) which controls the trade-off between maximizing the margin and minimizing misclassification errors.\n",
        "\n",
        "A low C value tolerates more errors, leading to a wider margin but potentially underfitting the data.\n",
        "\n",
        "A high C value penalizes errors more severely, resulting in a narrower margin but a stricter separation, similar to a Hard Margin SVM.\n",
        "\n",
        "Analogy: Using the same marble analogy, a Soft Margin SVM allows you to place the ruler with a few marbles slightly on the wrong side or within a \"buffer zone\" to get a better, more generalized separation overall.\n",
        "\n",
        "In summary, Hard Margin SVM aims for perfection but is fragile, while Soft Margin SVM is more realistic and robust by balancing the goal of a wide margin with the reality of imperfect data."
      ],
      "metadata": {
        "id": "kB_Mv9NY6AQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "Ans:The Kernel Trick is a powerful technique used by Support Vector Machines (SVMs) to handle non-linearly separable data without explicitly transforming the data into a higher-dimensional space.  Instead of performing the computationally expensive task of mapping the data, the kernel function calculates the dot product of the data points as if they were already in that higher-dimensional space. This allows SVM to find a linear decision boundary in the higher dimension, which translates to a non-linear boundary in the original feature space.\n",
        "\n",
        "Example: Radial Basis Function (RBF) Kernel\n",
        "The Radial Basis Function (RBF) kernel is one of the most popular and versatile kernels used with SVM.\n",
        "\n",
        "Formula: The RBF kernel is defined as:\n",
        "K(x,x\n",
        "′\n",
        " )=exp(−γ∣∣x−x\n",
        "′\n",
        " ∣∣\n",
        "2\n",
        " )\n",
        "where ∣∣x−x\n",
        "′\n",
        " ∣∣\n",
        "2\n",
        "  is the squared Euclidean distance between two points, and γ is a hyperparameter that controls the influence of each support vector.\n",
        "\n",
        "Use Case: The RBF kernel is particularly effective for datasets where the classes are arranged in complex, non-linear patterns. For example, consider a classification problem where the data points of one class form a circle in the center of data points from another class. A simple linear boundary would fail. The RBF kernel, however, can handle this by essentially creating a circular decision boundary, as it measures the proximity of data points to a central point. It is a good default choice for many classification problems as it can handle a wide variety of non-linear relationships."
      ],
      "metadata": {
        "id": "GeMy27JM6JqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "Ans:A Naïve Bayes classifier is a family of simple and probabilistic supervised machine learning algorithms used for classification. It's based on Bayes' Theorem and operates under a strong, simplifying assumption: that all features are independent of each other. This means the presence or absence of one feature does not affect the presence or absence of any other feature.\n",
        "\n",
        "\n",
        "\n",
        "Why it’s called “naïve”\n",
        "The \"naïve\" part of the name comes directly from this simplifying assumption of independence. In reality, this assumption is almost always false. For example, a classifier predicting whether a fruit is an orange might consider its features like \"round shape,\" \"orange color,\" and \"citrus smell.\" A Naïve Bayes classifier would assume that the orange color is completely independent of the round shape, which isn't true.\n",
        "\n",
        "\n",
        "However, despite this unrealistic assumption, Naïve Bayes classifiers often perform surprisingly well in practice, especially for tasks like text classification, spam filtering, and sentiment analysis. The simplicity of the model makes it computationally efficient and fast to train.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tKfRtyUU6lsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?\n",
        "Ans:While all Naïve Bayes classifiers are based on the same core principle of Bayes' Theorem and the \"naïve\" assumption of feature independence, they differ in how they model the distribution of the features. The choice of variant depends on the type of data you're working with.\n",
        "\n",
        "Gaussian Naïve Bayes\n",
        "The Gaussian Naïve Bayes variant assumes that the features follow a normal (Gaussian) distribution. This is particularly useful for continuous numerical data. It calculates the mean and variance for each feature within each class to estimate the probability of a data point belonging to a certain class.\n",
        "\n",
        "\n",
        "\n",
        "When to use it: Use Gaussian Naïve Bayes when your features are continuous, such as height, weight, temperature, or other measurements that can be described by a bell-shaped curve. It's often applied in fields like medical diagnosis or fraud detection where features are numerical.\n",
        "\n",
        "\n",
        "Multinomial Naïve Bayes\n",
        "The Multinomial Naïve Bayes variant is designed for data where the features are discrete counts. It models the probability of observing these counts. For example, in text classification, a document can be represented by a vector of word counts.\n",
        "\n",
        "\n",
        "When to use it: This is the go-to classifier for text classification problems like spam filtering, sentiment analysis, and document categorization. It works with features that represent the frequency of events, such as how many times a particular word appears in a document.\n",
        "\n",
        "\n",
        "Bernoulli Naïve Bayes\n",
        "The Bernoulli Naïve Bayes variant is also suited for discrete data, but it's specifically for binary features. It models the presence or absence of a feature, not its frequency. The feature vector is a binary vector (0s and 1s) indicating whether a specific word or feature exists in a document or not.\n",
        "\n",
        "\n",
        "When to use it: This is ideal for tasks where the simple presence or absence of a feature is more important than its frequency. A common use case is also text classification, but when the model should only consider if a word is present, not how many times it appears."
      ],
      "metadata": {
        "id": "Fik6cNcs60WN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors"
      ],
      "metadata": {
        "id": "hTJK9Gke7MFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans:\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target # Target variable (species)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "# We'll use 80% of the data for training and 20% for testing\n",
        "# random_state ensures reproducibility of the split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Train an SVM Classifier with a linear kernel\n",
        "# SVC stands for Support Vector Classification\n",
        "# kernel='linear' specifies a linear decision boundary\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# 3. Print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# 4. Print the support vectors\n",
        "# Support vectors are the data points closest to the hyperplane\n",
        "# They play a crucial role in defining the decision boundary\n",
        "print(\"\\nSupport Vectors:\")\n",
        "# svm_model.support_vectors_ contains the actual support vectors\n",
        "print(svm_model.support_vectors_)\n",
        "\n",
        "# You can also get the indices of the support vectors from the training set\n",
        "print(\"\\nIndices of Support Vectors (from training set):\")\n",
        "print(svm_model.support_)\n"
      ],
      "metadata": {
        "id": "UHx2ujfj7TY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score.\n"
      ],
      "metadata": {
        "id": "oIC6BwoL8BBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans:\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "# This dataset is commonly used for binary classification tasks.\n",
        "# It contains features computed from a digitized image of a fine needle aspirate (FNA)\n",
        "# of a breast mass, describing characteristics of the cell nuclei present in the image.\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data  # Features\n",
        "y = breast_cancer.target # Target variable (malignant or benign)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "# We'll use 80% of the data for training and 20% for testing.\n",
        "# random_state ensures reproducibility of the split, meaning you'll get the same split\n",
        "# every time you run the code with the same random_state value.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Train a Gaussian Naïve Bayes model\n",
        "# Gaussian Naïve Bayes is a probabilistic classifier based on Bayes' theorem\n",
        "# with the assumption of independence among predictors.\n",
        "# It assumes that the features follow a Gaussian (normal) distribution.\n",
        "gnb_model = GaussianNB()\n",
        "\n",
        "# Fit the model to the training data\n",
        "# This step trains the model using the provided training features (X_train)\n",
        "# and their corresponding target labels (y_train).\n",
        "gnb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "# Once the model is trained, we use it to predict the target labels\n",
        "# for the unseen test data (X_test).\n",
        "y_pred = gnb_model.predict(X_test)\n",
        "\n",
        "# 3. Print its classification report including precision, recall, and F1-score.\n",
        "# The classification report is a text summary of the precision, recall, F1-score\n",
        "# for each class, and support (number of occurrences of each class in y_test).\n",
        "# Precision: The ratio of correctly predicted positive observations to the total predicted positives.\n",
        "# Recall (Sensitivity): The ratio of correctly predicted positive observations to all observations in actual class.\n",
        "# F1-score: The weighted average of Precision and Recall. It tries to find the balance\n",
        "# between precision and recall.\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=breast_cancer.target_names))\n"
      ],
      "metadata": {
        "id": "hYTfn43H8IAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy"
      ],
      "metadata": {
        "id": "SCFATT2n8X2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans:# Import necessary libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "# The Wine dataset is a classic dataset for classification,\n",
        "# containing chemical analysis of wines grown in the same region in Italy\n",
        "# but derived from three different cultivars.\n",
        "wine = load_wine()\n",
        "X = wine.data  # Features (chemical properties)\n",
        "y = wine.target # Target variable (wine cultivar)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "# A test size of 20% is used, and random_state ensures reproducibility.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "# 'C' is the regularization parameter, controlling the trade-off between\n",
        "# achieving a low training error and a low testing error (generalization ability).\n",
        "# 'gamma' defines how much influence a single training example has.\n",
        "# Small gamma means a large influence, large gamma means small influence.\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],  # Common values for C\n",
        "    'gamma': [1, 0.1, 0.01, 0.001], # Common values for gamma\n",
        "    'kernel': ['rbf'] # We'll use the Radial Basis Function (RBF) kernel, which is common with gamma\n",
        "}\n",
        "\n",
        "# Create an SVM Classifier model\n",
        "# We start with a basic SVC model. GridSearchCV will tune its parameters.\n",
        "svm_model = SVC(random_state=42)\n",
        "\n",
        "# 2. Train an SVM Classifier using GridSearchCV to find the best C and gamma\n",
        "# GridSearchCV exhaustively searches over specified parameter values for an estimator.\n",
        "# cv=5 means 5-fold cross-validation will be used on the training data.\n",
        "# verbose=3 provides a detailed output during the search process.\n",
        "grid_search = GridSearchCV(svm_model, param_grid, cv=5, verbose=3, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "# This step performs the cross-validation and hyperparameter tuning.\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 3. Print the best hyperparameters and accuracy\n",
        "print(\"\\nBest Hyperparameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Get the best estimator (model with the best hyperparameters)\n",
        "best_svm_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred = best_svm_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the best model on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy of the best SVM model on the test set: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "Fp81bo7R8mBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions."
      ],
      "metadata": {
        "id": "jAA-gTuw-E77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans:# Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load a synthetic text dataset (20 Newsgroups)\n",
        "# We'll select two categories to make it a binary classification problem for ROC-AUC calculation.\n",
        "# Choosing 'alt.atheism' and 'soc.religion.christian' as they are distinct.\n",
        "categories = ['alt.atheism', 'soc.religion.christian']\n",
        "newsgroups_data = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "X_text = newsgroups_data.data  # The raw text documents\n",
        "y = newsgroups_data.target     # The target labels (0 for alt.atheism, 1 for soc.religion.christian)\n",
        "\n",
        "# Convert text data into numerical features using TF-IDF Vectorizer\n",
        "# TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic\n",
        "# that is intended to reflect how important a word is to a document in a collection or corpus.\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000) # Limit features for efficiency\n",
        "X = vectorizer.fit_transform(X_text)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "# 80% for training, 20% for testing, with random_state for reproducibility.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Train a Naïve Bayes Classifier\n",
        "# Multinomial Naïve Bayes is well-suited for classification with discrete features\n",
        "# (like word counts or TF-IDF values in text classification).\n",
        "nb_model = MultinomialNB()\n",
        "\n",
        "# Fit the model to the training data\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# 3. Print the model's ROC-AUC score for its predictions.\n",
        "# ROC-AUC (Receiver Operating Characteristic - Area Under the Curve) is a performance metric\n",
        "# for binary classifiers. It represents the ability of the model to distinguish between classes.\n",
        "# A higher AUC indicates a better model.\n",
        "# We need probability estimates for ROC-AUC. predict_proba returns the probability of each class.\n",
        "# For binary classification, we usually take the probability of the positive class (class 1).\n",
        "y_pred_proba = nb_model.predict_proba(X_test)[:, 1] # Probability of the positive class (soc.religion.christian)\n",
        "\n",
        "# Calculate the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(f\"Model's ROC-AUC Score: {roc_auc:.2f}\")\n"
      ],
      "metadata": {
        "id": "ARm9x7fo-FnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution."
      ],
      "metadata": {
        "id": "kPfun7tQ-UXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# --- 1. Create a Synthetic Text Dataset (Simulating Emails) ---\n",
        "# This dataset simulates an imbalanced scenario: 10 legitimate emails, 2 spam emails.\n",
        "# In a real scenario, you'd load actual email data.\n",
        "emails = [\n",
        "    \"Hello team, please find the meeting minutes attached. Regards, John.\", # Legitimate (0)\n",
        "    \"Hi, just a reminder about our project deadline next Friday.\",         # Legitimate (0)\n",
        "    \"Your invoice is ready for download. Click here to view.\",             # Spam (1) - suspicious link\n",
        "    \"Meeting rescheduled to 3 PM today. See you there.\",                   # Legitimate (0)\n",
        "    \"Congratulations! You've won a free iPhone. Claim your prize now!\",   # Spam (1) - typical scam\n",
        "    \"Regarding your recent inquiry, here is the information you requested.\",# Legitimate (0)\n",
        "    \"Weekly report for Q2 is due by end of day.\",                          # Legitimate (0)\n",
        "    \"Important security update: Verify your account details immediately.\", # Legitimate (0) - could be spam, but let's assume legitimate for this example\n",
        "    \"Please review the attached document and provide your feedback.\",      # Legitimate (0)\n",
        "    \"Confirm your email address to avoid account suspension.\",             # Legitimate (0) - could be spam, but let's assume legitimate for this example\n",
        "    \"New policy document has been uploaded to the shared drive.\",          # Legitimate (0)\n",
        "    \"Exclusive offer: Limited time discount on all products!\",            # Legitimate (0) - marketing, not spam for this example\n",
        "]\n",
        "\n",
        "# 0 for Not Spam, 1 for Spam\n",
        "labels = [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "# --- 2. Preprocess the Data (Text Vectorization) ---\n",
        "# TF-IDF Vectorizer to convert text into numerical features.\n",
        "# max_features limits the vocabulary size, stop_words removes common words.\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "X = vectorizer.fit_transform(emails)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "# Using a small test size due to the small synthetic dataset.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "print(\"--- Data Preprocessing Complete ---\")\n",
        "print(f\"Original data points: {len(emails)}\")\n",
        "print(f\"Training data points: {X_train.shape[0]}, Test data points: {X_test.shape[0]}\")\n",
        "print(f\"Training label distribution (0: Not Spam, 1: Spam): {np.bincount(y_train)}\")\n",
        "print(f\"Test label distribution (0: Not Spam, 1: Spam): {np.bincount(y_test)}\")\n",
        "\n",
        "# --- 3. Choose and Justify an Appropriate Model (Multinomial Naïve Bayes) ---\n",
        "# Multinomial Naïve Bayes is well-suited for text classification with TF-IDF features.\n",
        "# It's efficient and provides a good baseline.\n",
        "\n",
        "# --- 4. Address Class Imbalance ---\n",
        "# Calculate class weights to give more importance to the minority class (Spam).\n",
        "# This helps the model learn from the underrepresented class more effectively.\n",
        "# The 'balanced' mode automatically adjusts weights inversely proportional to class frequencies.\n",
        "nb_model = MultinomialNB(class_weight='balanced')\n",
        "\n",
        "# Train the Naïve Bayes model\n",
        "print(\"\\n--- Training Naïve Bayes Model ---\")\n",
        "nb_model.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- 5. Evaluate the Performance of Your Solution ---\n",
        "# Make predictions on the test set\n",
        "y_pred = nb_model.predict(X_test)\n",
        "\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred)) # Shows TP, TN, FP, FN\n",
        "\n",
        "# Print the classification report including precision, recall, and F1-score\n",
        "# target_names provides readable labels for the classes.\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Not Spam', 'Spam']))\n",
        "\n",
        "# Note: For a very small synthetic dataset, results might not be perfectly indicative\n",
        "# of real-world performance. Real datasets require more extensive preprocessing\n",
        "# and potentially more advanced techniques for imbalance and feature engineering.\n"
      ],
      "metadata": {
        "id": "XMt7nQi3-5wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ToB5XbnM--e_"
      }
    }
  ]
}